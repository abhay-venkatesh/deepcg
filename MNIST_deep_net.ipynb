{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inputs and outputs\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[784, None])\n",
    "y_true = tf.placeholder(tf.float32, shape=[10, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extracting Useful features\n",
    "\n",
    "# Weight and bias definition\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# Convolution and max pooling definitions\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Re-shape image in the required format # 28 x 28 x 1\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "# First Conv + Max-pool layer # 28 x 28 x 32, # 14 x 14 x 32\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# Second Conv + Max-pool layer # 14 x 14 x 64, # 7 x 7 x 64\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# Final fully connected layer to get good features # input: 1 x (7x7x64), output: 1 x 1024\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# Drop-out for the final layer\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(10), Dimension(None)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final Read-out layer and prediction\n",
    "W = weight_variable([10, 1024])\n",
    "b = bias_variable([10,1])\n",
    "y_pred = tf.matmul(W, tf.transpose(h_fc1_drop)) + b\n",
    "y_pred.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tf_frobenius_norm(M):\n",
    "    return tf.reduce_sum(M ** 2) ** 0.5\n",
    "\n",
    "# to implement nuclear norm\n",
    "def tf_nuclear_norm(M):\n",
    "    st, ut, vt = tf.svd(M,  full_matrices = False)\n",
    "    #st2 = tf.diag(st)\n",
    "    #st_r = tf.matmul(ut, tf.matmul(st2, tf.transpose(vt)))\n",
    "    #print('vish', ut.shape, st2.shape, tf.transpose(vt).shape, st_r.shape)\n",
    "    \n",
    "    uk = tf.reshape(ut[:, 0], [10, 1])\n",
    "    vk = tf.reshape(vt[:, 0], [1, 784])\n",
    "    sk = tf.matmul(uk, vk)\n",
    "    #sk = st[0] * sk\n",
    "    #print(st.shape, ut.shape, vt.shape)\n",
    "    #print('before', type(sk), sk.shape)\n",
    "    return sk, _, _, _\n",
    "\n",
    "# def tf_nuclear_norm_pm(M):\n",
    "#     un = np.zeros([10, 1])\n",
    "#     un[0][0] = 1\n",
    "#     u = tf.constant(un)\n",
    "#     vn = np.zeros([1, 784])\n",
    "#     vn[0][0] = 1\n",
    "#     v = tf.constant(vn)\n",
    "#     for _ in range(20): \n",
    "#         u = tf.matmul(M, x)\n",
    "#         v = tf.\n",
    "\n",
    "def Sgdnm(grad, wt):\n",
    "    return (grad / tf_frobenius_norm(grad))\n",
    "\n",
    "def Cgd_Fn(grad, wt):\n",
    "    return ((1 - alpha ) / alpha) * (wt + lam1 * grad / tf_frobenius_norm(grad))\n",
    "\n",
    "def Cgd_Nn(grad, wt):\n",
    "    nn, st, st_r, M = tf_nuclear_norm(grad)\n",
    "    return ((1 - alpha ) / alpha) * (wt - lam2 * nn), st, st_r, M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred))\n",
    "\n",
    "# hyper-parameters\n",
    "# alpha = tf.placeholder_with_default(tf.constant(0.5), tf.constant(0.001).shape)\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "start_train = 0.00001 # Requires very high lambda for Cgd_Fn\n",
    "# k=1, start_train = 1, decay_rate = 1 ---> 1/t learning rate\n",
    "k = 1\n",
    "alpha = tf.train.inverse_time_decay(start_train, global_step, k, 0)\n",
    "lam1 = tf.placeholder_with_default(tf.constant(4.0), tf.constant(10000.0).shape)\n",
    "lam2 = tf.placeholder_with_default(tf.constant(4.0), tf.constant(100.0).shape)\n",
    "\n",
    "# Gradient Descent optimizer\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate = alpha)\n",
    "\n",
    "# Compute the gradients for a list of variables.\n",
    "grads_and_vars = opt.compute_gradients(loss)\n",
    "\n",
    "# SGD update\n",
    "gv_sgd = [(gv[0], gv[1]) for gv in grads_and_vars]\n",
    "optimizer_gv_sgd = opt.apply_gradients(gv_sgd, global_step=global_step)\n",
    "g0_sgd = grads_and_vars[0][0]\n",
    "w0_sgd = grads_and_vars[0][1]\n",
    "s0_sgd = w0_sgd\n",
    "w1_sgd = w0_sgd - alpha * s0_sgd\n",
    "\n",
    "\n",
    "# Normalized SGD update\n",
    "gv_nsgd = [(Sgdnm(gv[0], gv[1]), gv[1]) for gv in grads_and_vars]\n",
    "optimizer_gv_nsgd = opt.apply_gradients(gv_nsgd, global_step=global_step)\n",
    "g0_nsgd = grads_and_vars[0][0]\n",
    "w0_nsgd = grads_and_vars[0][1]\n",
    "s0_nsgd = Sgdnm(g0_nsgd, w0_nsgd)\n",
    "w1_nsgd = w0_nsgd - alpha * s0_nsgd\n",
    "\n",
    "\n",
    "# CGD with FN\n",
    "gv_cgd_fn = [(Cgd_Fn(gv[0], gv[1]), gv[1]) for gv in grads_and_vars]\n",
    "optimizer_gv_cgd_fn = opt.apply_gradients(gv_cgd_fn, global_step=global_step)\n",
    "g0_cgd_fn = grads_and_vars[0][0]\n",
    "w0_cgd_fn = grads_and_vars[0][1]\n",
    "s0_cgd_fn = Cgd_Fn(g0_cgd_fn, w0_cgd_fn)\n",
    "w1_cgd_fn = w0_cgd_fn - alpha * s0_cgd_fn\n",
    "\n",
    "# CGD with NN\n",
    "# g0_cgd_nn = grads_and_vars[0][0]\n",
    "# w0_cgd_nn = grads_and_vars[0][1]\n",
    "# s0_cgd_nn, st, st_r, M = Cgd_Nn(g0_cgd_nn, w0_cgd_nn)\n",
    "# gv_cgd_nn = [(s0_cgd_nn, gv[1]) for gv in grads_and_vars]\n",
    "# optimizer_gv_cgd_nn = opt.apply_gradients(gv_cgd_nn, global_step=global_step)\n",
    "# w1_cgd_nn = w0_cgd_nn - alpha * s0_cgd_nn\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_pred, 0), tf.argmax(y_true, 0))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalized gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy= 0.14 loss= 64.6559\n",
      "NSgd iterates: w(t+1) = 2.45905 w(t) = 2.45905 s(t) = 1.0\n",
      "alpha [9.9999997e-06]\n",
      "train_accuracy= 0.12 loss= 67.7933\n",
      "NSgd iterates: w(t+1) = 2.45905 w(t) = 2.45905 s(t) = 1.0\n",
      "alpha [9.9999997e-06]\n",
      "train_accuracy= 0.08 loss= 61.91\n",
      "NSgd iterates: w(t+1) = 2.45905 w(t) = 2.45905 s(t) = 1.0\n",
      "alpha [9.9999997e-06]\n",
      "train_accuracy= 0.12 loss= 63.503\n",
      "NSgd iterates: w(t+1) = 2.45905 w(t) = 2.45905 s(t) = 1.0\n",
      "alpha [9.9999997e-06]\n",
      "train_accuracy= 0.1 loss= 68.4093\n",
      "NSgd iterates: w(t+1) = 2.45905 w(t) = 2.45905 s(t) = 1.0\n",
      "alpha [9.9999997e-06]\n",
      "train_accuracy= 0.06 loss= 64.8341\n",
      "NSgd iterates: w(t+1) = 2.45905 w(t) = 2.45905 s(t) = 1.0\n",
      "alpha [9.9999997e-06]\n",
      "train_accuracy= 0.14 loss= 67.0881\n",
      "NSgd iterates: w(t+1) = 2.45905 w(t) = 2.45905 s(t) = 1.0\n",
      "alpha [9.9999997e-06]\n",
      "train_accuracy= 0.09 loss= 67.2228\n",
      "NSgd iterates: w(t+1) = 2.45905 w(t) = 2.45905 s(t) = 1.0\n",
      "alpha [9.9999997e-06]\n",
      "train_accuracy= 0.1 loss= 67.4117\n",
      "NSgd iterates: w(t+1) = 2.45905 w(t) = 2.45905 s(t) = 1.0\n",
      "alpha [9.9999997e-06]\n",
      "train_accuracy= 0.11 loss= 66.4731\n",
      "NSgd iterates: w(t+1) = 2.45905 w(t) = 2.45905 s(t) = 1.0\n",
      "alpha [9.9999997e-06]\n",
      "test_accuracy= 0.11\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(10):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        feed_dict_keepall = {x: np.transpose(batch[0]), y_true: np.transpose(batch[1]), keep_prob: 1}\n",
    "        feed_dict_keepsome = {x: np.transpose(batch[0]), y_true: np.transpose(batch[1]), keep_prob: 0.5}\n",
    "        train_accuracy, loss_, gv_, W_ = sess.run([accuracy, loss, gv_nsgd, W], feed_dict_keepall)\n",
    "        w1_nsgd_, w0_nsgd_, s0_nsgd_= sess.run([w1_nsgd, w0_nsgd, s0_nsgd], feed_dict_keepall)\n",
    "        alpha_ = sess.run([alpha], feed_dict)\n",
    "            \n",
    "        if i % 1 == 0:\n",
    "            print('train_accuracy=', train_accuracy, 'loss=', loss_)\n",
    "            print('NSgd iterates: w(t+1) =', LA.norm(w1_nsgd_), 'w(t) =', LA.norm(w0_nsgd_), 's(t) =', LA.norm(s0_nsgd_))\n",
    "            print('alpha', alpha_)\n",
    "        sess.run(optimizer_gv_nsgd, feed_dict_keepsome)\n",
    "    \n",
    "    feed_dict={x: np.transpose(mnist.test.images), y_true: np.transpose(mnist.test.labels)}\n",
    "    test_accuracy = sess.run(accuracy, feed_dict_keepall)\n",
    "    print('test_accuracy=', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frobenius norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     for i in range(1000):\n",
    "#         batch = mnist.train.next_batch(100)\n",
    "#         batch1 = batch[:]\n",
    "#         feed_dict = {x: np.transpose(batch[0]), y_true: np.transpose(batch[1])}\n",
    "#         train_accuracy, loss_, gv_, W1_ = sess.run([accuracy, loss, gv_cgd_fn, W], feed_dict)\n",
    "#         w1_cgd_fn_, w0_cgd_fn_, s0_cgd_fn_ = sess.run([w1_cgd_fn, w0_cgd_fn, s0_cgd_fn], feed_dict)\n",
    "#         alpha_ = sess.run([alpha], feed_dict)\n",
    "            \n",
    "#         if i % 100 == 0:\n",
    "#             print('train_accuracy=', train_accuracy, 'loss value =',loss_)\n",
    "#             print('frob_nrom of iterates: w(t+1) =', LA.norm(w1_cgd_fn_), 'w(t) =', LA.norm(w0_cgd_fn_), 's(t) =', LA.norm(s0_cgd_fn_))\n",
    "#             print('alpha', alpha_)\n",
    "#         sess.run(optimizer_gv_cgd_fn, feed_dict)\n",
    "    \n",
    "#     feed_dict={x: np.transpose(mnist.test.images), y_true: np.transpose(mnist.test.labels)}\n",
    "#     test_accuracy = sess.run(accuracy, feed_dict)\n",
    "#     print('test_accuracy', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Nuclear Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     for i in range(100):\n",
    "#         batch = mnist.train.next_batch(100)\n",
    "#         batch2 = batch[:]\n",
    "#         feed_dict = {x: np.transpose(batch[0]), y_true: np.transpose(batch[1])}\n",
    "#         train_accuracy, loss_, gv_, W2_ = sess.run([accuracy, loss, gv_cgd_nn, W], feed_dict)\n",
    "#         w1_cgd_nn_, w0_cgd_nn_, s0_cgd_nn_, st_, st_r_, M_ = sess.run([w1_cgd_nn, w0_cgd_nn, s0_cgd_nn, st, st_r, M], feed_dict)\n",
    "#         alpha_ = sess.run([alpha], feed_dict)\n",
    "            \n",
    "#         if i % 1 == 0:\n",
    "#             print('train_accuracy=', train_accuracy, 'loss value =',loss_)\n",
    "#             print('nuclear_norm of iterates: w(t+1) =', LA.norm(w1_cgd_nn_), 'w(t) =', LA.norm(w0_cgd_nn_), 's(t) =', LA.norm(s0_cgd_nn_))\n",
    "#             print('alpha', alpha_)\n",
    "#         sess.run(optimizer_gv_cgd_nn, feed_dict)\n",
    "    \n",
    "#     feed_dict={x: np.transpose(mnist.test.images), y_true: np.transpose(mnist.test.labels)}\n",
    "#     test_accuracy = sess.run(accuracy, feed_dict)\n",
    "#     print('test_accuracy', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LA.norm(st_r_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
