{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(10), Dimension(None)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[784, None])\n",
    "y_true = tf.placeholder(tf.float32, shape=[10, None])\n",
    "W = tf.Variable(tf.zeros([10,784]))\n",
    "# b = tf.Variable(tf.zeros([10,1]))\n",
    "y_pred = tf.matmul(W, x) \n",
    "y_pred.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tf_frobenius_norm(M):\n",
    "    return tf.reduce_sum(M ** 2) ** 0.5\n",
    "\n",
    "# to implement nuclear norm\n",
    "def tf_nuclear_norm(M):\n",
    "    st, ut, vt = tf.svd(M)\n",
    "    uk = tf.reshape(ut[:, 0], [10, 1])\n",
    "    vk = tf.reshape(vt[:, 0], [1, 784])\n",
    "    sk = tf.matmul(uk, vk)\n",
    "    print(sk.shape)\n",
    "    return sk\n",
    "\n",
    "def Sgdnm(grad, wt):\n",
    "    return (grad / tf_frobenius_norm(grad))\n",
    "\n",
    "def Cgd_Fn(grad, wt):\n",
    "    return ((1 - alpha ) / alpha) * (wt + lam1 * grad / tf_frobenius_norm(grad))\n",
    "\n",
    "def Cgd_Nn(grad, wt):\n",
    "    return ((1 - alpha ) / alpha) * (wt + lam2 * tf_nuclear_norm(grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 784)\n",
      "(10, 784)\n"
     ]
    }
   ],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred))\n",
    "\n",
    "# hyper-parameters\n",
    "# alpha = tf.placeholder_with_default(tf.constant(0.5), tf.constant(0.001).shape)\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "start_train = 0.9999 # Requires very high lambda for Cgd_Fn\n",
    "# k=1, start_train = 1, decay_rate = 1 ---> 1/t learning rate\n",
    "k = 1\n",
    "alpha = tf.train.inverse_time_decay(start_train, global_step, k, 0)\n",
    "lam1 = tf.placeholder_with_default(tf.constant(4.0), tf.constant(10000.0).shape)\n",
    "lam2 = tf.placeholder_with_default(tf.constant(4.0), tf.constant(100.0).shape)\n",
    "\n",
    "# Gradient Descent optimizer\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate = alpha)\n",
    "\n",
    "# Compute the gradients for a list of variables.\n",
    "grads_and_vars = opt.compute_gradients(loss)\n",
    "\n",
    "# SGD update\n",
    "gv_sgd = [(gv[0], gv[1]) for gv in grads_and_vars]\n",
    "optimizer_gv_sgd = opt.apply_gradients(gv_sgd, global_step=global_step)\n",
    "g0_sgd = grads_and_vars[0][0]\n",
    "w0_sgd = grads_and_vars[0][1]\n",
    "s0_sgd = w0_sgd\n",
    "w1_sgd = w0_sgd - alpha * s0_sgd\n",
    "\n",
    "# Normalized SGD update\n",
    "gv_nsgd = [(Sgdnm(gv[0], gv[1]), gv[1]) for gv in grads_and_vars]\n",
    "optimizer_gv_nsgd = opt.apply_gradients(gv_nsgd, global_step=global_step)\n",
    "g0_nsgd = grads_and_vars[0][0]\n",
    "w0_nsgd = grads_and_vars[0][1]\n",
    "s0_nsgd = Sgdnm(g0_nsgd, w0_nsgd)\n",
    "w1_nsgd = w0_nsgd - alpha * s0_nsgd\n",
    "\n",
    "\n",
    "# CGD with FN\n",
    "gv_cgd_fn = [(Cgd_Fn(gv[0], gv[1]), gv[1]) for gv in grads_and_vars]\n",
    "optimizer_gv_cgd_fn = opt.apply_gradients(gv_cgd_fn, global_step=global_step)\n",
    "g0_cgd_fn = grads_and_vars[0][0]\n",
    "w0_cgd_fn = grads_and_vars[0][1]\n",
    "s0_cgd_fn = Cgd_Fn(g0_cgd_fn, w0_cgd_fn)\n",
    "w1_cgd_fn = w0_cgd_fn - alpha * s0_cgd_fn\n",
    "\n",
    "# CGD with NN\n",
    "gv_cgd_nn = [(Cgd_Nn(gv[0], gv[1]), gv[1]) for gv in grads_and_vars]\n",
    "optimizer_gv_cgd_nn = opt.apply_gradients(gv_cgd_nn, global_step=global_step)\n",
    "g0_cgd_nn = grads_and_vars[0][0]\n",
    "w0_cgd_nn = grads_and_vars[0][1]\n",
    "s0_cgd_nn = Cgd_Nn(g0_cgd_nn, w0_cgd_nn)\n",
    "w1_cgd_nn = w0_cgd_nn - alpha * s0_cgd_nn\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_pred, 0), tf.argmax(y_true, 0))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalized gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     for i in range(1000):\n",
    "#         batch = mnist.train.next_batch(100)\n",
    "#         feed_dict = {x: np.transpose(batch[0]), y_true: np.transpose(batch[1])}\n",
    "#         train_accuracy, loss_, gv_, W_ = sess.run([accuracy, loss, gv_nsgd, W], feed_dict)\n",
    "#         w1_nsgd_, w0_nsgd_, s0_nsgd_= sess.run([w1_nsgd, w0_nsgd, s0_nsgd], feed_dict)\n",
    "#         alpha_ = sess.run([alpha], feed_dict)\n",
    "            \n",
    "#         if i % 1 == 0:\n",
    "#             print('train_accuracy=', train_accuracy, 'loss=', loss_)\n",
    "#             print('NSgd iterates: w(t+1) =', LA.norm(w1_nsgd_), 'w(t) =', LA.norm(w0_nsgd_), 's(t) =', LA.norm(s0_nsgd_))\n",
    "#             print('alpha', alpha_)\n",
    "#         sess.run(optimizer_gv_nsgd, feed_dict)\n",
    "    \n",
    "#     feed_dict={x: np.transpose(mnist.test.images), y_true: np.transpose(mnist.test.labels)}\n",
    "#     test_accuracy = sess.run(accuracy, feed_dict)\n",
    "#     print('test_accuracy=', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frobenius norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     for i in range(1000):\n",
    "#         batch = mnist.train.next_batch(100)\n",
    "#         feed_dict = {x: np.transpose(batch[0]), y_true: np.transpose(batch[1])}\n",
    "#         train_accuracy, loss_, gv_, W_ = sess.run([accuracy, loss, gv_cgd_fn, W], feed_dict)\n",
    "#         w1_cgd_fn_, w0_cgd_fn_, s0_cgd_fn_ = sess.run([w1_cgd_fn, w0_cgd_fn, s0_cgd_fn], feed_dict)\n",
    "#         alpha_ = sess.run([alpha], feed_dict)\n",
    "            \n",
    "#         if i % 100 == 0:\n",
    "#             print('train_accuracy=', train_accuracy, 'loss value =',loss_)\n",
    "#             print('frob_nrom of iterates: w(t+1) =', LA.norm(w1_cgd_fn_), 'w(t) =', LA.norm(w0_cgd_fn_), 's(t) =', LA.norm(s0_cgd_fn_))\n",
    "#             print('alpha', alpha_)\n",
    "#         sess.run(optimizer_gv_cgd_fn, feed_dict)\n",
    "    \n",
    "#     feed_dict={x: np.transpose(mnist.test.images), y_true: np.transpose(mnist.test.labels)}\n",
    "#     test_accuracy = sess.run(accuracy, feed_dict)\n",
    "#     print('test_accuracy', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Nuclear Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy= 0.06 loss value = 46.0517\n",
      "frob_nrom of iterates: w(t+1) = 0.000400067 w(t) = 0.0 s(t) = 0.000400107\n",
      "alpha [0.99989998]\n",
      "train_accuracy= 0.08 loss value = 46.0735\n",
      "frob_nrom of iterates: w(t+1) = 0.0373263 w(t) = 0.036969 s(t) = 0.000396777\n",
      "alpha [0.99989998]\n",
      "train_accuracy= 0.15 loss value = 46.1121\n",
      "frob_nrom of iterates: w(t+1) = 0.0737596 w(t) = 0.0734108 s(t) = 0.000393587\n",
      "alpha [0.99989998]\n",
      "train_accuracy= 0.16 loss value = 46.1402\n",
      "frob_nrom of iterates: w(t+1) = 0.10975 w(t) = 0.10938 s(t) = 0.000389707\n",
      "alpha [0.99989998]\n",
      "train_accuracy= 0.12 loss value = 46.1569\n",
      "frob_nrom of iterates: w(t+1) = 0.145223 w(t) = 0.144899 s(t) = 0.000387931\n",
      "alpha [0.99989998]\n",
      "train_accuracy= 0.07 loss value = 46.1707\n",
      "frob_nrom of iterates: w(t+1) = 0.18079 w(t) = 0.180432 s(t) = 0.000383211\n",
      "alpha [0.99989998]\n",
      "train_accuracy= 0.09 loss value = 46.2533\n",
      "frob_nrom of iterates: w(t+1) = 0.215905 w(t) = 0.21554 s(t) = 0.00037933\n",
      "alpha [0.99989998]\n",
      "train_accuracy= 0.09 loss value = 46.3086\n",
      "frob_nrom of iterates: w(t+1) = 0.250416 w(t) = 0.250077 s(t) = 0.000377505\n",
      "alpha [0.99989998]\n",
      "train_accuracy= 0.17 loss value = 46.3224\n",
      "frob_nrom of iterates: w(t+1) = 0.284803 w(t) = 0.284462 s(t) = 0.00037398\n",
      "alpha [0.99989998]\n",
      "train_accuracy= 0.18 loss value = 46.4838\n",
      "frob_nrom of iterates: w(t+1) = 0.3189 w(t) = 0.318575 s(t) = 0.00037203\n",
      "alpha [0.99989998]\n",
      "test_accuracy 0.1135\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(1000):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        feed_dict = {x: np.transpose(batch[0]), y_true: np.transpose(batch[1])}\n",
    "        train_accuracy, loss_, gv_, W_ = sess.run([accuracy, loss, gv_cgd_nn, W], feed_dict)\n",
    "        w1_cgd_nn_, w0_cgd_nn_, s0_cgd_nn_ = sess.run([w1_cgd_nn, w0_cgd_nn, s0_cgd_nn], feed_dict)\n",
    "        alpha_ = sess.run([alpha], feed_dict)\n",
    "            \n",
    "        if i % 100 == 0:\n",
    "            print('train_accuracy=', train_accuracy, 'loss value =',loss_)\n",
    "            print('frob_nrom of iterates: w(t+1) =', LA.norm(w1_cgd_nn_), 'w(t) =', LA.norm(w0_cgd_nn_), 's(t) =', LA.norm(s0_cgd_nn_))\n",
    "            print('alpha', alpha_)\n",
    "        sess.run(optimizer_gv_cgd_nn, feed_dict)\n",
    "    \n",
    "    feed_dict={x: np.transpose(mnist.test.images), y_true: np.transpose(mnist.test.labels)}\n",
    "    test_accuracy = sess.run(accuracy, feed_dict)\n",
    "    print('test_accuracy', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
