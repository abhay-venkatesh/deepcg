{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(10), Dimension(None)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[784, None])\n",
    "y_true = tf.placeholder(tf.float32, shape=[10, None])\n",
    "W = tf.Variable(tf.zeros([10,784]))\n",
    "# b = tf.Variable(tf.zeros([10,1]))\n",
    "y_pred = tf.matmul(W, x) \n",
    "y_pred.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred))\n",
    "# optimzr = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "# correct_prediction = tf.equal(tf.argmax(y_pred, 0), tf.argmax(y_true, 0))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tf_frobenius_norm(M):\n",
    "    return tf.reduce_sum(M ** 2) ** 0.5\n",
    "\n",
    "# # to implement nuclear norm\n",
    "# def tf_nuclear_norm(M):\n",
    "#     st, ut, vt = tf.svd(M)\n",
    "#     uk = ut[:, 0]\n",
    "#     vk = vt[:, 0]\n",
    "#     print(M.shape, ut.shape, vt.shape, tf.transpose(vk).shape)\n",
    "#     sk = tf.matmul(uk, tf.transpose(vk))\n",
    "#     return sk\n",
    "\n",
    "def Sgdnm(grad, wt):\n",
    "    return (grad / tf_frobenius_norm(grad))\n",
    "\n",
    "def Cgd_Fn(grad, wt):\n",
    "    return ((1 - alpha ) / alpha) * (wt + lam1 * grad / tf_frobenius_norm(grad))\n",
    "\n",
    "def Cgd_Nn(grad, wt):\n",
    "    return ((1 - alpha ) / alpha) * (wt - lam2 * tf_nuclear_norm(grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred))\n",
    "\n",
    "# hyper-parameters\n",
    "#alpha = tf.placeholder_with_default(tf.constant(0.5), tf.constant(0.001).shape)\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "start_train = 0.999\n",
    "# k=1, start_train = 1, decay_rate = 1 ---> 1/t learning rate\n",
    "k = 1\n",
    "alpha = tf.train.inverse_time_decay(start_train, global_step, k, 0)\n",
    "lam1 = tf.placeholder_with_default(tf.constant(4.0), tf.constant(10000.0).shape)\n",
    "lam2 = tf.placeholder_with_default(tf.constant(100.0), tf.constant(100.0).shape)\n",
    "\n",
    "# Gradient Descent optimizer\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate = alpha)\n",
    "\n",
    "# Compute the gradients for a list of variables.\n",
    "grads_and_vars = opt.compute_gradients(loss)\n",
    "\n",
    "# SGD update\n",
    "# gv_sgd = [(gv[0], gv[1]) for gv in grads_and_vars]\n",
    "# optimizer_gv_sgd = opt.apply_gradients(gv_sgd, global_step=global_step)\n",
    "# qa = grads_and_vars[0][0]\n",
    "# qb = tf_frobenius_norm(grads_and_vars[0][0])\n",
    "# qc = grads_and_vars[0][0] / tf_frobenius_norm(grads_and_vars[0][0])\n",
    "# qd = Cgd_Fn(grads_and_vars[0][0], grads_and_vars[0][1])\n",
    "# qe = grads_and_vars[0][1]\n",
    "# qf = ((1 - alpha ) / alpha)*(grads_and_vars[0][1] - lam1 * grads_and_vars[0][0] / tf_frobenius_norm(grads_and_vars[0][0]))\n",
    "# qg = qe - alpha * qa\n",
    "\n",
    "# # Normalized SGD update\n",
    "# gv_nsgd = [(Sgdnm(gv[0], gv[1]), gv[1]) for gv in grads_and_vars]\n",
    "# optimizer_gv_nsgd = opt.apply_gradients(gv_nsgd, global_step=global_step)\n",
    "# qa = grads_and_vars[0][0]\n",
    "# qb = tf_frobenius_norm(grads_and_vars[0][0])\n",
    "# qc = grads_and_vars[0][0] / tf_frobenius_norm(grads_and_vars[0][0])\n",
    "# qd = Cgd_Fn(grads_and_vars[0][0], grads_and_vars[0][1])\n",
    "# qe = grads_and_vars[0][1]\n",
    "# qf = Sgdnm(grads_and_vars[0][0], grads_and_vars[0][1])\n",
    "# qg = qe - alpha * qf\n",
    "\n",
    "\n",
    "# CGD with FN\n",
    "gv_cgd_fn = [(Cgd_Fn(gv[0], gv[1]), gv[1]) for gv in grads_and_vars]\n",
    "optimizer_gv_cgd_fn = opt.apply_gradients(gv_cgd_fn, global_step=global_step)\n",
    "qa = grads_and_vars[0][0]\n",
    "qb = tf_frobenius_norm(grads_and_vars[0][0])\n",
    "qc = grads_and_vars[0][0] / tf_frobenius_norm(grads_and_vars[0][0])\n",
    "qd = Cgd_Fn(grads_and_vars[0][0], grads_and_vars[0][1])\n",
    "qe = grads_and_vars[0][1]\n",
    "qf = ((1 - alpha ) / alpha)*(grads_and_vars[0][1] - lam1 * grads_and_vars[0][0] / tf_frobenius_norm(grads_and_vars[0][0]))\n",
    "qg = qe - alpha * qf\n",
    "\n",
    "# # CGD with NN\n",
    "# gv_cgd_nn = [(Cgd_Nn(gv[0], gv[1]), gv[1]) for gv in grads_and_vars]\n",
    "# optimizer_gv_cgd_Nn = opt.apply_gradients(gv_cgd_nn)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_pred, 0), tf.argmax(y_true, 0))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nuclear Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     for i in range(1000):\n",
    "#         batch = mnist.train.next_batch(100)\n",
    "#         feed_dict = {x: np.transpose(batch[0]), y_true: np.transpose(batch[1])}\n",
    "#         train_accuracy, loss_, gv_, W_ = sess.run([accuracy, loss, gv_cgd_nn, W], feed_dict)\n",
    "# #         qa_,qb_,qc_,qd_,qe_,qf_, qg_ = sess.run([qa,qb,qc,qd,qe,qf,qg], feed_dict)\n",
    "            \n",
    "#         if i % 100 == 0:\n",
    "#             print('train_accuracy', train_accuracy, loss_)\n",
    "# #             print('frob_nrom: qg =', LA.norm(qg_), 'qe =', LA.norm(qe_), 'qf =', LA.norm(qf_))\n",
    "# #             print('gradient', LA.norm(qa_))\n",
    "#         sess.run(optimizer_gv_cgd_nn, feed_dict)\n",
    "    \n",
    "#     feed_dict={x: np.transpose(mnist.test.images), y_true: np.transpose(mnist.test.labels)}\n",
    "#     test_accuracy = sess.run(accuracy, feed_dict)\n",
    "#     print('test_accuracy', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frobenius norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy= 0.1 loss value = 46.0517\n",
      "frob_nrom of iterates: w(t+1) = 0.00399995 w(t) = 0.0 s(t) = 0.00400395\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.71 loss value = 44.3283\n",
      "frob_nrom of iterates: w(t+1) = 0.342185 w(t) = 0.346091 s(t) = 0.00431553\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.7 loss value = 42.8548\n",
      "frob_nrom of iterates: w(t+1) = 0.655422 w(t) = 0.659551 s(t) = 0.00458885\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.63 loss value = 42.2044\n",
      "frob_nrom of iterates: w(t+1) = 0.937463 w(t) = 0.942003 s(t) = 0.00486987\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.65 loss value = 41.9642\n",
      "frob_nrom of iterates: w(t+1) = 1.19232 w(t) = 1.19713 s(t) = 0.00511397\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.65 loss value = 41.5802\n",
      "frob_nrom of iterates: w(t+1) = 1.4237 w(t) = 1.42881 s(t) = 0.0053486\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.71 loss value = 41.9958\n",
      "frob_nrom of iterates: w(t+1) = 1.63274 w(t) = 1.63787 s(t) = 0.00549534\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.71 loss value = 41.8092\n",
      "frob_nrom of iterates: w(t+1) = 1.82258 w(t) = 1.82794 s(t) = 0.00568434\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.65 loss value = 43.3205\n",
      "frob_nrom of iterates: w(t+1) = 1.99218 w(t) = 1.9978 s(t) = 0.00587916\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.73 loss value = 44.0823\n",
      "frob_nrom of iterates: w(t+1) = 2.14661 w(t) = 2.1524 s(t) = 0.00602994\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.63 loss value = 42.9499\n",
      "frob_nrom of iterates: w(t+1) = 2.28621 w(t) = 2.29212 s(t) = 0.00615735\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.67 loss value = 42.6029\n",
      "frob_nrom of iterates: w(t+1) = 2.41211 w(t) = 2.41814 s(t) = 0.00627733\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.75 loss value = 42.7725\n",
      "frob_nrom of iterates: w(t+1) = 2.52579 w(t) = 2.53185 s(t) = 0.00635365\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.65 loss value = 42.7849\n",
      "frob_nrom of iterates: w(t+1) = 2.62755 w(t) = 2.63383 s(t) = 0.00649782\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.69 loss value = 44.6634\n",
      "frob_nrom of iterates: w(t+1) = 2.72205 w(t) = 2.72846 s(t) = 0.006605\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.55 loss value = 43.6441\n",
      "frob_nrom of iterates: w(t+1) = 2.80556 w(t) = 2.81188 s(t) = 0.00661156\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.69 loss value = 44.0368\n",
      "frob_nrom of iterates: w(t+1) = 2.8813 w(t) = 2.88778 s(t) = 0.00672007\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.65 loss value = 43.5423\n",
      "frob_nrom of iterates: w(t+1) = 2.95155 w(t) = 2.95811 s(t) = 0.00679223\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.71 loss value = 43.1988\n",
      "frob_nrom of iterates: w(t+1) = 3.01314 w(t) = 3.01991 s(t) = 0.00692053\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.68 loss value = 44.7495\n",
      "frob_nrom of iterates: w(t+1) = 3.06995 w(t) = 3.07667 s(t) = 0.00692916\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.62 loss value = 50.7455\n",
      "frob_nrom of iterates: w(t+1) = 3.12056 w(t) = 3.12726 s(t) = 0.00694806\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.66 loss value = 50.4431\n",
      "frob_nrom of iterates: w(t+1) = 3.16726 w(t) = 3.174 s(t) = 0.00698515\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.69 loss value = 43.4696\n",
      "frob_nrom of iterates: w(t+1) = 3.20952 w(t) = 3.21635 s(t) = 0.00704625\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.64 loss value = 52.5661\n",
      "frob_nrom of iterates: w(t+1) = 3.24747 w(t) = 3.25443 s(t) = 0.00712499\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.65 loss value = 45.0678\n",
      "frob_nrom of iterates: w(t+1) = 3.28116 w(t) = 3.28806 s(t) = 0.00711883\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.68 loss value = 48.9805\n",
      "frob_nrom of iterates: w(t+1) = 3.31213 w(t) = 3.31907 s(t) = 0.00715343\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.65 loss value = 45.2649\n",
      "frob_nrom of iterates: w(t+1) = 3.33952 w(t) = 3.34651 s(t) = 0.00719285\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.71 loss value = 48.4625\n",
      "frob_nrom of iterates: w(t+1) = 3.36381 w(t) = 3.37088 s(t) = 0.00723905\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.66 loss value = 47.2546\n",
      "frob_nrom of iterates: w(t+1) = 3.38638 w(t) = 3.39334 s(t) = 0.0071988\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.71 loss value = 44.4432\n",
      "frob_nrom of iterates: w(t+1) = 3.40663 w(t) = 3.41362 s(t) = 0.00722607\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.67 loss value = 45.8651\n",
      "frob_nrom of iterates: w(t+1) = 3.42459 w(t) = 3.43175 s(t) = 0.00731161\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.67 loss value = 49.9052\n",
      "frob_nrom of iterates: w(t+1) = 3.44021 w(t) = 3.44724 s(t) = 0.00725921\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.63 loss value = 55.333\n",
      "frob_nrom of iterates: w(t+1) = 3.45613 w(t) = 3.4633 s(t) = 0.00733477\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.72 loss value = 43.5269\n",
      "frob_nrom of iterates: w(t+1) = 3.47042 w(t) = 3.4775 s(t) = 0.00729378\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.67 loss value = 47.2118\n",
      "frob_nrom of iterates: w(t+1) = 3.48334 w(t) = 3.49033 s(t) = 0.00726245\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.66 loss value = 51.8594\n",
      "frob_nrom of iterates: w(t+1) = 3.49439 w(t) = 3.50145 s(t) = 0.00729999\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.63 loss value = 50.5414\n",
      "frob_nrom of iterates: w(t+1) = 3.50541 w(t) = 3.51245 s(t) = 0.00729568\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.66 loss value = 44.1777\n",
      "frob_nrom of iterates: w(t+1) = 3.51519 w(t) = 3.52231 s(t) = 0.00733908\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.73 loss value = 48.5543\n",
      "frob_nrom of iterates: w(t+1) = 3.52261 w(t) = 3.52984 s(t) = 0.00739467\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.65 loss value = 53.6577\n",
      "frob_nrom of iterates: w(t+1) = 3.5302 w(t) = 3.53734 s(t) = 0.00735721\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.67 loss value = 51.7629\n",
      "frob_nrom of iterates: w(t+1) = 3.53551 w(t) = 3.54268 s(t) = 0.00737232\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.62 loss value = 47.1587\n",
      "frob_nrom of iterates: w(t+1) = 3.54138 w(t) = 3.54862 s(t) = 0.0074084\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.71 loss value = 54.1981\n",
      "frob_nrom of iterates: w(t+1) = 3.54627 w(t) = 3.55345 s(t) = 0.00738632\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.63 loss value = 49.0362\n",
      "frob_nrom of iterates: w(t+1) = 3.55206 w(t) = 3.55922 s(t) = 0.00737619\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.7 loss value = 49.7439\n",
      "frob_nrom of iterates: w(t+1) = 3.55865 w(t) = 3.56584 s(t) = 0.00739356\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.69 loss value = 52.6374\n",
      "frob_nrom of iterates: w(t+1) = 3.56277 w(t) = 3.57 s(t) = 0.00741638\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.67 loss value = 51.6393\n",
      "frob_nrom of iterates: w(t+1) = 3.56656 w(t) = 3.57377 s(t) = 0.00740604\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.73 loss value = 51.9035\n",
      "frob_nrom of iterates: w(t+1) = 3.56964 w(t) = 3.57692 s(t) = 0.00744552\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.63 loss value = 54.0765\n",
      "frob_nrom of iterates: w(t+1) = 3.57277 w(t) = 3.57993 s(t) = 0.00738695\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.63 loss value = 45.8057\n",
      "frob_nrom of iterates: w(t+1) = 3.57485 w(t) = 3.5821 s(t) = 0.0074338\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.61 loss value = 49.4146\n",
      "frob_nrom of iterates: w(t+1) = 3.57693 w(t) = 3.58415 s(t) = 0.00742066\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.68 loss value = 49.4512\n",
      "frob_nrom of iterates: w(t+1) = 3.57796 w(t) = 3.58517 s(t) = 0.0074108\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.64 loss value = 48.1073\n",
      "frob_nrom of iterates: w(t+1) = 3.57951 w(t) = 3.58679 s(t) = 0.00744622\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.65 loss value = 48.8624\n",
      "frob_nrom of iterates: w(t+1) = 3.58124 w(t) = 3.58847 s(t) = 0.00742226\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.73 loss value = 45.0395\n",
      "frob_nrom of iterates: w(t+1) = 3.58464 w(t) = 3.59187 s(t) = 0.00742399\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.58 loss value = 48.3865\n",
      "frob_nrom of iterates: w(t+1) = 3.58678 w(t) = 3.59398 s(t) = 0.00741062\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.67 loss value = 45.682\n",
      "frob_nrom of iterates: w(t+1) = 3.58829 w(t) = 3.5955 s(t) = 0.00742049\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.7 loss value = 48.8175\n",
      "frob_nrom of iterates: w(t+1) = 3.59107 w(t) = 3.59837 s(t) = 0.00746413\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.69 loss value = 47.3809\n",
      "frob_nrom of iterates: w(t+1) = 3.59239 w(t) = 3.59975 s(t) = 0.00749326\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.71 loss value = 50.2178\n",
      "frob_nrom of iterates: w(t+1) = 3.59305 w(t) = 3.60034 s(t) = 0.0074561\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.69 loss value = 54.633\n",
      "frob_nrom of iterates: w(t+1) = 3.59457 w(t) = 3.60179 s(t) = 0.00742765\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.56 loss value = 49.3544\n",
      "frob_nrom of iterates: w(t+1) = 3.59609 w(t) = 3.60337 s(t) = 0.00745787\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.68 loss value = 50.1738\n",
      "frob_nrom of iterates: w(t+1) = 3.59748 w(t) = 3.60457 s(t) = 0.00736488\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.68 loss value = 46.4149\n",
      "frob_nrom of iterates: w(t+1) = 3.59715 w(t) = 3.60431 s(t) = 0.00739963\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.69 loss value = 47.2262\n",
      "frob_nrom of iterates: w(t+1) = 3.59806 w(t) = 3.60533 s(t) = 0.00745318\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.66 loss value = 52.6379\n",
      "frob_nrom of iterates: w(t+1) = 3.59848 w(t) = 3.60566 s(t) = 0.00740439\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.71 loss value = 56.7969\n",
      "frob_nrom of iterates: w(t+1) = 3.59912 w(t) = 3.60637 s(t) = 0.0074422\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.71 loss value = 48.4703\n",
      "frob_nrom of iterates: w(t+1) = 3.5994 w(t) = 3.6066 s(t) = 0.00742238\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.68 loss value = 46.8839\n",
      "frob_nrom of iterates: w(t+1) = 3.59937 w(t) = 3.60674 s(t) = 0.00750216\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.62 loss value = 51.7707\n",
      "frob_nrom of iterates: w(t+1) = 3.59967 w(t) = 3.60673 s(t) = 0.0073504\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.62 loss value = 56.2921\n",
      "frob_nrom of iterates: w(t+1) = 3.59902 w(t) = 3.60625 s(t) = 0.00742963\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.66 loss value = 51.0522\n",
      "frob_nrom of iterates: w(t+1) = 3.59914 w(t) = 3.60632 s(t) = 0.00741001\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.64 loss value = 45.6879\n",
      "frob_nrom of iterates: w(t+1) = 3.59897 w(t) = 3.60619 s(t) = 0.0074261\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.6 loss value = 48.1321\n",
      "frob_nrom of iterates: w(t+1) = 3.59981 w(t) = 3.60694 s(t) = 0.00738517\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.78 loss value = 45.2537\n",
      "frob_nrom of iterates: w(t+1) = 3.59986 w(t) = 3.60709 s(t) = 0.00743533\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.77 loss value = 50.6834\n",
      "frob_nrom of iterates: w(t+1) = 3.60004 w(t) = 3.60717 s(t) = 0.00738482\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.71 loss value = 49.1\n",
      "frob_nrom of iterates: w(t+1) = 3.59964 w(t) = 3.60684 s(t) = 0.00741683\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.56 loss value = 47.0127\n",
      "frob_nrom of iterates: w(t+1) = 3.60039 w(t) = 3.60763 s(t) = 0.00743935\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.7 loss value = 50.7115\n",
      "frob_nrom of iterates: w(t+1) = 3.59871 w(t) = 3.60597 s(t) = 0.007447\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.69 loss value = 50.8668\n",
      "frob_nrom of iterates: w(t+1) = 3.59866 w(t) = 3.60592 s(t) = 0.00744627\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.64 loss value = 47.8064\n",
      "frob_nrom of iterates: w(t+1) = 3.5987 w(t) = 3.60602 s(t) = 0.00747674\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.59 loss value = 44.1294\n",
      "frob_nrom of iterates: w(t+1) = 3.5991 w(t) = 3.60619 s(t) = 0.00736474\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.65 loss value = 45.6395\n",
      "frob_nrom of iterates: w(t+1) = 3.59886 w(t) = 3.60612 s(t) = 0.00745134\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.77 loss value = 46.1085\n",
      "frob_nrom of iterates: w(t+1) = 3.59863 w(t) = 3.60584 s(t) = 0.00742535\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.6 loss value = 52.123\n",
      "frob_nrom of iterates: w(t+1) = 3.59836 w(t) = 3.60559 s(t) = 0.00742999\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.72 loss value = 46.2873\n",
      "frob_nrom of iterates: w(t+1) = 3.59828 w(t) = 3.60547 s(t) = 0.00741197\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.68 loss value = 50.8086\n",
      "frob_nrom of iterates: w(t+1) = 3.59895 w(t) = 3.60619 s(t) = 0.0074404\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.62 loss value = 44.896\n",
      "frob_nrom of iterates: w(t+1) = 3.5992 w(t) = 3.60649 s(t) = 0.00746407\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.68 loss value = 46.7513\n",
      "frob_nrom of iterates: w(t+1) = 3.60102 w(t) = 3.60835 s(t) = 0.00748015\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.64 loss value = 49.2544\n",
      "frob_nrom of iterates: w(t+1) = 3.60246 w(t) = 3.6097 s(t) = 0.00744295\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.7 loss value = 50.5009\n",
      "frob_nrom of iterates: w(t+1) = 3.60245 w(t) = 3.60976 s(t) = 0.00747615\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.67 loss value = 44.2491\n",
      "frob_nrom of iterates: w(t+1) = 3.60327 w(t) = 3.6105 s(t) = 0.00743603\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.71 loss value = 49.8777\n",
      "frob_nrom of iterates: w(t+1) = 3.60311 w(t) = 3.61028 s(t) = 0.00740544\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.62 loss value = 49.8232\n",
      "frob_nrom of iterates: w(t+1) = 3.60349 w(t) = 3.61066 s(t) = 0.00740823\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.66 loss value = 47.6191\n",
      "frob_nrom of iterates: w(t+1) = 3.60308 w(t) = 3.61029 s(t) = 0.00742317\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.65 loss value = 50.7839\n",
      "frob_nrom of iterates: w(t+1) = 3.60112 w(t) = 3.60833 s(t) = 0.0074204\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.61 loss value = 53.7693\n",
      "frob_nrom of iterates: w(t+1) = 3.6012 w(t) = 3.60853 s(t) = 0.00748161\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.69 loss value = 46.8719\n",
      "frob_nrom of iterates: w(t+1) = 3.6022 w(t) = 3.60949 s(t) = 0.0074642\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.64 loss value = 50.4133\n",
      "frob_nrom of iterates: w(t+1) = 3.6024 w(t) = 3.60972 s(t) = 0.00748122\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.69 loss value = 52.5294\n",
      "frob_nrom of iterates: w(t+1) = 3.60205 w(t) = 3.6092 s(t) = 0.00739466\n",
      "alpha [0.99900001]\n",
      "test_accuracy 0.6809\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(10000):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        feed_dict = {x: np.transpose(batch[0]), y_true: np.transpose(batch[1])}\n",
    "        train_accuracy, loss_, gv_, W_ = sess.run([accuracy, loss, gv_cgd_fn, W], feed_dict)\n",
    "        qa_,qb_,qc_,qd_,qe_,qf_, qg_ = sess.run([qa,qb,qc,qd,qe,qf,qg], feed_dict)\n",
    "        alpha_ = sess.run([alpha], feed_dict)\n",
    "            \n",
    "        if i % 100 == 0:\n",
    "            print('train_accuracy=', train_accuracy, 'loss value =',loss_)\n",
    "            print('frob_nrom of iterates: w(t+1) =', LA.norm(qg_), 'w(t) =', LA.norm(qe_), 's(t) =', LA.norm(qf_))\n",
    "            print('alpha', alpha_)\n",
    "#             print('gradient', LA.norm(qa_))\n",
    "        sess.run(optimizer_gv_cgd_fn, feed_dict)\n",
    "    \n",
    "    feed_dict={x: np.transpose(mnist.test.images), y_true: np.transpose(mnist.test.labels)}\n",
    "    test_accuracy = sess.run(accuracy, feed_dict)\n",
    "    print('test_accuracy', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
