{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inputs and outputs\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Useful features\n",
    "\n",
    "# Weight and bias definition\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# Convolution and max pooling definitions\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Re-shape image in the required format # x_image: ? x (28x28x1)\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "# First Conv + Max-pool layer #h_conv1: ? x (28x28x32), #h_pool1 ? x (14x14x32)\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# Second Conv + Max-pool layer #h_conv2: ? x (14x14x64), #h_pool2 ? x (7x7x64)\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# Final fully connected layer to get good features #h_pool2: ? x (7x7x64), #h_fc1: 1 x 1024\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# Drop-out for the final layer\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(W.shape, h_fc1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1024) (1024, 10) (?, 10)\n"
     ]
    }
   ],
   "source": [
    "# Final Read-out layer and prediction\n",
    "W = weight_variable([1024, 10])\n",
    "b = bias_variable([10])\n",
    "y_pred = tf.matmul(h_fc1_drop, W) + b\n",
    "print(h_fc1_drop.shape, W.shape, y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_frobenius_norm(M):\n",
    "    return tf.reduce_sum(M ** 2) ** 0.5\n",
    "\n",
    "# # to implement nuclear norm\n",
    "# def tf_nuclear_norm(M):\n",
    "#     st, ut, vt = tf.svd(M,  full_matrices = False)\n",
    "#     #st2 = tf.diag(st)\n",
    "#     #st_r = tf.matmul(ut, tf.matmul(st2, tf.transpose(vt)))\n",
    "#     #print('vish', ut.shape, st2.shape, tf.transpose(vt).shape, st_r.shape)\n",
    "    \n",
    "#     uk = tf.reshape(ut[:, 0], [10, 1])\n",
    "#     vk = tf.reshape(vt[:, 0], [1, 784])\n",
    "#     sk = tf.matmul(uk, vk)\n",
    "#     #sk = st[0] * sk\n",
    "#     #print(st.shape, ut.shape, vt.shape)\n",
    "#     #print('before', type(sk), sk.shape)\n",
    "#     return sk, _, _, _\n",
    "\n",
    "# # def tf_nuclear_norm_pm(M):\n",
    "# #     un = np.zeros([10, 1])\n",
    "# #     un[0][0] = 1\n",
    "# #     u = tf.constant(un)\n",
    "# #     vn = np.zeros([1, 784])\n",
    "# #     vn[0][0] = 1\n",
    "# #     v = tf.constant(vn)\n",
    "# #     for _ in range(20): \n",
    "# #         u = tf.matmul(M, x)\n",
    "# #         v = tf.\n",
    "\n",
    "def Sgdnm(grad, wt):\n",
    "    return grad #(grad / tf_frobenius_norm(grad))\n",
    "\n",
    "def Cgd_Fn(grad, wt):\n",
    "    return ((1 - alpha ) / alpha) * (wt + lam1 * grad / tf_frobenius_norm(grad))\n",
    "\n",
    "# def Cgd_Nn(grad, wt):\n",
    "#     nn, st, st_r, M = tf_nuclear_norm(grad)\n",
    "#     return ((1 - alpha ) / alpha) * (wt - lam2 * nn), st, st_r, M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "\n",
    "\n",
    "# choice of alpha values\n",
    "# adam = 1e-5\n",
    "# SGD = 0.01\n",
    "# NSGD = ?\n",
    "# CGD_FN = 0.99990, \n",
    "# CGD_NN =?\n",
    "\n",
    "# Alpha\n",
    "# alpha = tf.placeholder_with_default(tf.constant(0.5), tf.constant(0.001).shape)\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "start_train = 0.9990 # Requires very high lambda for Cgd_Fn\n",
    "# # k=1, start_train = 1, decay_rate = 1 ---> 1/t learning rate\n",
    "k = 1\n",
    "alpha = tf.train.inverse_time_decay(start_train, global_step, k, 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Lambda\n",
    "lam1 = tf.placeholder_with_default(tf.constant(4.0), tf.constant(4.0).shape)\n",
    "# lam2 = tf.placeholder_with_default(tf.constant(4.0), tf.constant(100.0).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 10) (?, 10) (?, 784)\n"
     ]
    }
   ],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred))\n",
    "\n",
    "\n",
    "# Adam optimizer\n",
    "opt_adam = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "# Gradient Descent optimizer\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate = alpha)\n",
    "\n",
    "# Compute the gradients for a list of variables.\n",
    "grads_and_vars = opt.compute_gradients(loss)\n",
    "\n",
    "# # SGD update\n",
    "# gv_sgd = [(gv[0], gv[1]) for gv in grads_and_vars]\n",
    "# optimizer_gv_sgd = opt.apply_gradients(gv_sgd, global_step=global_step)\n",
    "# g0_sgd = grads_and_vars[0][0]\n",
    "# w0_sgd = grads_and_vars[0][1]\n",
    "# s0_sgd = w0_sgd\n",
    "# w1_sgd = w0_sgd - alpha * s0_sgd\n",
    "\n",
    "\n",
    "# # Normalized SGD update\n",
    "# gv_nsgd = [(Sgdnm(gv[0], gv[1]), gv[1]) for gv in grads_and_vars]\n",
    "# optimizer_gv_nsgd = opt.apply_gradients(gv_nsgd, global_step=global_step)\n",
    "# g0_nsgd = grads_and_vars[0][0]\n",
    "# w0_nsgd = grads_and_vars[0][1]\n",
    "# s0_nsgd = Sgdnm(g0_nsgd, w0_nsgd)\n",
    "# w1_nsgd = w0_nsgd - alpha * s0_nsgd\n",
    "\n",
    "\n",
    "# CGD with FN\n",
    "gv_cgd_fn = [(Cgd_Fn(gv[0], gv[1]), gv[1]) for gv in grads_and_vars]\n",
    "optimizer_gv_cgd_fn = opt.apply_gradients(gv_cgd_fn, global_step=global_step)\n",
    "g0_cgd_fn = grads_and_vars[0][0]\n",
    "w0_cgd_fn = grads_and_vars[0][1]\n",
    "s0_cgd_fn = Cgd_Fn(g0_cgd_fn, w0_cgd_fn)\n",
    "w1_cgd_fn = w0_cgd_fn - alpha * s0_cgd_fn\n",
    "\n",
    "# # CGD with NN\n",
    "# # g0_cgd_nn = grads_and_vars[0][0]\n",
    "# # w0_cgd_nn = grads_and_vars[0][1]\n",
    "# # s0_cgd_nn, st, st_r, M = Cgd_Nn(g0_cgd_nn, w0_cgd_nn)\n",
    "# # gv_cgd_nn = [(s0_cgd_nn, gv[1]) for gv in grads_and_vars]\n",
    "# # optimizer_gv_cgd_nn = opt.apply_gradients(gv_cgd_nn, global_step=global_step)\n",
    "# # w1_cgd_nn = w0_cgd_nn - alpha * s0_cgd_nn\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(y_pred.shape, y_true.shape, x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     for i in range(20000):\n",
    "#         batch = mnist.train.next_batch(50)\n",
    "#         if i%100 == 0:\n",
    "#             train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_true: batch[1], keep_prob: 1.0})\n",
    "#             print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "#         opt_adam.run(feed_dict={x: batch[0], y_true: batch[1], keep_prob: 0.5})\n",
    "\n",
    "#     print(\"test accuracy %g\"%accuracy.eval(feed_dict={x: mnist.test.images, y_true: mnist.test.labels, keep_prob: 1.0}))\n",
    "#     y_pred_, y_true_ = sess.run([y_pred, y_true], feed_dict={x: mnist.test.images, y_true: mnist.test.labels, keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalized gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # config = tf.ConfigProto()\n",
    "# # config.gpu_options.allow_growth = True\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     for i in range(20000):\n",
    "#         batch = mnist.train.next_batch(50)\n",
    "#         feed_dict_keepall = {x: batch[0], y_true: batch[1], keep_prob: 1}\n",
    "#         feed_dict_keepsome = {x: batch[0], y_true: batch[1], keep_prob: 0.5}\n",
    "#         train_accuracy, loss_, gv_, W_ = sess.run([accuracy, loss, gv_nsgd, W], feed_dict_keepall)\n",
    "#         w1_nsgd_, w0_nsgd_, s0_nsgd_= sess.run([w1_nsgd, w0_nsgd, s0_nsgd], feed_dict_keepall)\n",
    "#         alpha_ = sess.run([alpha], feed_dict_keepall)\n",
    "            \n",
    "#         if i % 1000 == 0:\n",
    "#             print('train_accuracy=', train_accuracy, 'loss=', loss_)\n",
    "#             print('NSgd iterates: w(t+1) =', LA.norm(w1_nsgd_), 'w(t) =', LA.norm(w0_nsgd_), 's(t) =', LA.norm(s0_nsgd_))\n",
    "#             print('alpha', alpha_)\n",
    "#         sess.run(optimizer_gv_nsgd, feed_dict_keepsome)\n",
    "    \n",
    "#     feed_dict_test={x: mnist.test.images, y_true: mnist.test.labels, keep_prob: 1}\n",
    "#     test_accuracy = sess.run(accuracy, feed_dict_test)\n",
    "#     print('test_accuracy=', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frobenius norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy= 0.06 loss value = 8.78572\n",
      "frob_nrom of iterates: w(t+1) = 2.59158 w(t) = 2.59462 s(t) = 0.00500984\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.88 loss value = 0.835669\n",
      "frob_nrom of iterates: w(t+1) = 1.76142 w(t) = 1.76005 s(t) = 0.00284329\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.8 loss value = 0.760115\n",
      "frob_nrom of iterates: w(t+1) = 2.90586 w(t) = 2.90488 s(t) = 0.00137844\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.88 loss value = 0.675667\n",
      "frob_nrom of iterates: w(t+1) = 3.51869 w(t) = 3.5183 s(t) = 0.000959304\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.88 loss value = 0.69359\n",
      "frob_nrom of iterates: w(t+1) = 3.76158 w(t) = 3.76141 s(t) = 0.000753483\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.86 loss value = 0.687012\n",
      "frob_nrom of iterates: w(t+1) = 3.84939 w(t) = 3.8495 s(t) = 0.00142204\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.86 loss value = 0.593546\n",
      "frob_nrom of iterates: w(t+1) = 3.88368 w(t) = 3.88367 s(t) = 0.000898746\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.84 loss value = 0.746942\n",
      "frob_nrom of iterates: w(t+1) = 3.89567 w(t) = 3.89559 s(t) = 0.000476704\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.82 loss value = 0.790758\n",
      "frob_nrom of iterates: w(t+1) = 3.89904 w(t) = 3.89898 s(t) = 0.000552027\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.8 loss value = 0.738097\n",
      "frob_nrom of iterates: w(t+1) = 3.90139 w(t) = 3.90138 s(t) = 0.000841735\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.88 loss value = 0.68255\n",
      "frob_nrom of iterates: w(t+1) = 3.90315 w(t) = 3.90307 s(t) = 0.000367091\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.9 loss value = 0.534893\n",
      "frob_nrom of iterates: w(t+1) = 3.90233 w(t) = 3.90229 s(t) = 0.000713389\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.8 loss value = 0.779188\n",
      "frob_nrom of iterates: w(t+1) = 3.90013 w(t) = 3.90022 s(t) = 0.00122035\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.82 loss value = 0.770524\n",
      "frob_nrom of iterates: w(t+1) = 3.89909 w(t) = 3.89913 s(t) = 0.00105422\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.9 loss value = 0.618633\n",
      "frob_nrom of iterates: w(t+1) = 3.90159 w(t) = 3.90156 s(t) = 0.000696022\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.94 loss value = 0.579061\n",
      "frob_nrom of iterates: w(t+1) = 3.90101 w(t) = 3.90094 s(t) = 0.000492247\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.78 loss value = 0.813271\n",
      "frob_nrom of iterates: w(t+1) = 3.90608 w(t) = 3.90605 s(t) = 0.000702606\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.92 loss value = 0.567708\n",
      "frob_nrom of iterates: w(t+1) = 3.90169 w(t) = 3.90161 s(t) = 0.000417616\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.84 loss value = 0.648286\n",
      "frob_nrom of iterates: w(t+1) = 3.90005 w(t) = 3.9 s(t) = 0.000649999\n",
      "alpha [0.99900001]\n",
      "train_accuracy= 0.86 loss value = 0.72804\n",
      "frob_nrom of iterates: w(t+1) = 3.90219 w(t) = 3.90219 s(t) = 0.000885015\n",
      "alpha [0.99900001]\n",
      "test_accuracy 0.8679\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(20000):\n",
    "        batch = mnist.train.next_batch(50)\n",
    "        feed_dict_keepall = {x: batch[0], y_true: batch[1], keep_prob: 1}\n",
    "        feed_dict_keepsome = {x: batch[0], y_true: batch[1], keep_prob: 0.5}\n",
    "        train_accuracy, loss_, gv_, W1_ = sess.run([accuracy, loss, gv_cgd_fn, W], feed_dict_keepall)\n",
    "        w1_cgd_fn_, w0_cgd_fn_, s0_cgd_fn_ = sess.run([w1_cgd_fn, w0_cgd_fn, s0_cgd_fn], feed_dict_keepall)\n",
    "        alpha_ = sess.run([alpha], feed_dict_keepall)\n",
    "            \n",
    "        if i % 1000 == 0:\n",
    "            print('train_accuracy=', train_accuracy, 'loss value =',loss_)\n",
    "            print('frob_nrom of iterates: w(t+1) =', LA.norm(w1_cgd_fn_), 'w(t) =', LA.norm(w0_cgd_fn_), 's(t) =', LA.norm(s0_cgd_fn_))\n",
    "            print('alpha', alpha_)\n",
    "        sess.run(optimizer_gv_cgd_fn, feed_dict_keepsome)\n",
    "    \n",
    "    feed_dict_test={x: mnist.test.images, y_true: mnist.test.labels, keep_prob: 1}\n",
    "    test_accuracy = sess.run(accuracy, feed_dict_test)\n",
    "    print('test_accuracy', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nuclear Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(20000):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        feed_dict_keepall = {x: batch[0], y_true: batch[1], keep_prob: 1}\n",
    "        feed_dict_keepsome = {x: batch[0], y_true: batch[1], keep_prob: 0.5}\n",
    "        train_accuracy, loss_, gv_, W2_ = sess.run([accuracy, loss, gv_cgd_nn, W], feed_dict_keepall)\n",
    "        w1_cgd_nn_, w0_cgd_nn_, s0_cgd_nn_, st_, st_r_, M_ = sess.run([w1_cgd_nn, w0_cgd_nn, s0_cgd_nn, st, st_r, M], feed_dict_keepall)\n",
    "        alpha_ = sess.run([alpha], feed_dict_keepall)\n",
    "            \n",
    "        if i % 1000 == 0:\n",
    "            print('train_accuracy=', train_accuracy, 'loss value =',loss_)\n",
    "            print('nuclear_norm of iterates: w(t+1) =', LA.norm(w1_cgd_nn_), 'w(t) =', LA.norm(w0_cgd_nn_), 's(t) =', LA.norm(s0_cgd_nn_))\n",
    "            print('alpha', alpha_)\n",
    "        sess.run(optimizer_gv_cgd_nn, feed_dict_keepsome)\n",
    "    \n",
    "    feed_dict_test={x: mnist.test.images, y_true: mnist.test.labels, keep_prob: 1}\n",
    "    test_accuracy = sess.run(accuracy, feed_dict_test)\n",
    "    print('test_accuracy', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learning]",
   "language": "python",
   "name": "conda-env-deep-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
